{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lets import required packages\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.cross_validation import KFold,cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "\n",
    "San_train = pd.read_csv(r'E:\\Study\\Data Challenges\\kaggle\\Santander\\train\\train.csv')\n",
    "San_test = pd.read_csv(r'E:\\Study\\Data Challenges\\kaggle\\Santander\\test\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Checking for Categorical and numercial data and seperating the column names\n",
    "\n",
    "San_categorical = []\n",
    "San_numerical = []\n",
    "for col in San_train.columns:\n",
    "    if San_train[col].dtype == 'object':\n",
    "        San_categorical.append(col)\n",
    "    else:\n",
    "        San_numerical.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "San_categorical\n",
    "\n",
    "#Results show that complete data set is built of int or float values\n",
    "#So we dont need to use any dummies here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76020, 371)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### looking at the sahpe\n",
    "San_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column ID has all unique values so we will drop the column\n",
      "No of columns with all unique values are 1\n"
     ]
    }
   ],
   "source": [
    "#to find columns with unique values\n",
    "\n",
    "#Seperate our id columns from test to make sure that id columns exist for submission\n",
    "\n",
    "San_test_id = San_test['ID']\n",
    "\n",
    "San_Target =  San_train['TARGET']\n",
    "\n",
    "i=0\n",
    "for col in San_train.columns:\n",
    "    if len(np.unique(San_train[col].values))==76020:\n",
    "        print(\"Column %s has all unique values so we will drop the column\" %(col))\n",
    "        i=i+1\n",
    "        San_train = San_train.drop(col,1)\n",
    "        San_test = San_test.drop(col,1)\n",
    "print(\"No of columns with all unique values are %s\" %i)\n",
    "\n",
    "#But we are not going to delet the id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column ind_var2_0 has a constant value\n",
      "Column ind_var2 has a constant value\n",
      "Column ind_var27_0 has a constant value\n",
      "Column ind_var28_0 has a constant value\n",
      "Column ind_var28 has a constant value\n",
      "Column ind_var27 has a constant value\n",
      "Column ind_var41 has a constant value\n",
      "Column ind_var46_0 has a constant value\n",
      "Column ind_var46 has a constant value\n",
      "Column num_var27_0 has a constant value\n",
      "Column num_var28_0 has a constant value\n",
      "Column num_var28 has a constant value\n",
      "Column num_var27 has a constant value\n",
      "Column num_var41 has a constant value\n",
      "Column num_var46_0 has a constant value\n",
      "Column num_var46 has a constant value\n",
      "Column saldo_var28 has a constant value\n",
      "Column saldo_var27 has a constant value\n",
      "Column saldo_var41 has a constant value\n",
      "Column saldo_var46 has a constant value\n",
      "Column imp_amort_var18_hace3 has a constant value\n",
      "Column imp_amort_var34_hace3 has a constant value\n",
      "Column imp_reemb_var13_hace3 has a constant value\n",
      "Column imp_reemb_var33_hace3 has a constant value\n",
      "Column imp_trasp_var17_out_hace3 has a constant value\n",
      "Column imp_trasp_var33_out_hace3 has a constant value\n",
      "Column num_var2_0_ult1 has a constant value\n",
      "Column num_var2_ult1 has a constant value\n",
      "Column num_reemb_var13_hace3 has a constant value\n",
      "Column num_reemb_var33_hace3 has a constant value\n",
      "Column num_trasp_var17_out_hace3 has a constant value\n",
      "Column num_trasp_var33_out_hace3 has a constant value\n",
      "Column saldo_var2_ult1 has a constant value\n",
      "Column saldo_medio_var13_medio_hace3 has a constant value\n"
     ]
    }
   ],
   "source": [
    "#to find columns with constant valued volumns and remove them\n",
    "\n",
    "drop_train_col = []\n",
    "drop_test_col = []\n",
    "for col in San_train.columns:\n",
    "            if len(np.unique(San_train[col].values)) == 1:\n",
    "                San_train = San_train.drop(col,1)\n",
    "                San_test = San_test.drop(col,1)\n",
    "                drop_train_col.append(col)\n",
    "                drop_test_col.append(col)\n",
    "                print(\"Column %s has a constant value\" %(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET column is present in train but not present in test\n"
     ]
    }
   ],
   "source": [
    "# to check for columns whihc are present in train but not in test\n",
    "#Before doing so lets seperate our target varibale\n",
    "\n",
    "Target = San_train['TARGET']\n",
    "\n",
    "for col in San_train.columns:\n",
    "    if col not in San_test.columns:\n",
    "        San_train=San_train.drop(col,1)\n",
    "        print(\"%s column is present in train but not present in test\" %(col))\n",
    "        drop_train_col.append(col)     \n",
    "        \n",
    "        \n",
    "        \n",
    "#Similarly removing variables in test data which are not in train\n",
    "\n",
    "for col in San_test.columns:\n",
    "    if col not in San_train.columns:\n",
    "        San_train=San_test.drop(col,1)\n",
    "        print(\"%s column is present in train but not present in test\" %(col))\n",
    "        drop_test_col.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76020, 335)\n",
      "(75818, 335)\n",
      "76020\n",
      "75818\n"
     ]
    }
   ],
   "source": [
    "## Checking if the dataset after manipulation is in good shape or not\n",
    "\n",
    "\n",
    "print(San_train.shape)\n",
    "print(San_test.shape)\n",
    "print(len(San_Target))\n",
    "print(len(San_test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for any missing values\n",
    "\n",
    "San_train.isnull().sum().sum()\n",
    "\n",
    "San_test.isnull().sum().sum()\n",
    "\n",
    "#Output is 0 , so there are no missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 73012 number of 0 in the Target\n",
      "There are 3008 number of 1 in the Target\n"
     ]
    }
   ],
   "source": [
    "# To check if the dataset is balanced\n",
    "\n",
    "print(\"There are %s number of 0 in the Target\" %sum(San_Target==0))\n",
    "print(\"There are %s number of 1 in the Target\" %sum(San_Target==1))\n",
    "\n",
    "# output suggest that the dataset is highly imbalnce\n",
    "# so we have to balance it before building the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets try to random under sampling\n",
    "\n",
    "San_train['TARGET']=San_Target ##Adding back the target variable to the train dataset to do under sampling\n",
    "\n",
    "under_san_train = pd.concat([San_train[San_train['TARGET']==0].sample(n=3008,replace=False),San_train[San_train['TARGET']==1]])\n",
    "\n",
    "Target_under = under_san_train['TARGET']\n",
    "under_san_train = under_san_train.drop('TARGET',1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_es = 500\n",
      "max_f = 2\n",
      "0.725066445183\n",
      "n_es = 500\n",
      "max_f = 3\n",
      "0.7272303433\n",
      "n_es = 500\n",
      "max_f = 4\n",
      "0.726564784053\n",
      "n_es = 600\n",
      "max_f = 2\n",
      "0.723070874862\n",
      "n_es = 600\n",
      "max_f = 3\n",
      "0.725894795127\n",
      "n_es = 600\n",
      "max_f = 4\n",
      "0.724569213732\n",
      "n_es = 700\n",
      "max_f = 2\n",
      "0.724069767442\n",
      "n_es = 700\n",
      "max_f = 3\n",
      "0.725233665559\n",
      "n_es = 700\n",
      "max_f = 4\n",
      "0.726557585825\n",
      "n_es = 800\n",
      "max_f = 2\n",
      "0.726065337763\n",
      "n_es = 800\n",
      "max_f = 3\n",
      "0.720911406423\n",
      "n_es = 800\n",
      "max_f = 4\n",
      "0.725063122924\n",
      "n_es = 900\n",
      "max_f = 2\n",
      "0.724735326689\n",
      "n_es = 900\n",
      "max_f = 3\n",
      "0.727395348837\n",
      "n_es = 900\n",
      "max_f = 4\n",
      "0.726562015504\n"
     ]
    }
   ],
   "source": [
    "for n_es in range(500,1000,100):\n",
    "    for max_f in range(2,5,1):\n",
    "        rf = RandomForestClassifier(n_estimators = n_es,criterion ='entropy',max_features=max_f)\n",
    "        print(\"n_es = %s\"%n_es)\n",
    "        print(\"max_f = %s\"%max_f)\n",
    "        print(cross_val_score(rf,under_san_train,Target_under,cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Based on the best case achieved above we will build the random forest classifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators =900 ,criterion ='entropy',max_features=18)\n",
    "forest = forest.fit(under_san_train,Target_under)\n",
    "output_under = forest.predict(San_test)\n",
    "\n",
    "#Then to save the predicted output\n",
    "\n",
    "RF_1 = pd.DataFrame({'ID':San_test_id,'TARGET':output_under})\n",
    "\n",
    "os.chdir(r'E:\\Study\\Data Challenges\\kaggle\\Santander') #Changing the Dir to save the ouput file\n",
    "\n",
    "RF_1.to_csv(\"RF_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID        5752677905\n",
       "TARGET         21068\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets try to random over sampling\n",
    "\n",
    "over_san_train = pd.concat([San_train[San_train['TARGET']==1].sample(n=73012,replace=True),San_train[San_train['TARGET']==0]])\n",
    "\n",
    "Target_over = under_san_train['TARGET']\n",
    "over_san_train = under_san_train.drop('TARGET',1)\n",
    "\n",
    "### CV scoring\n",
    "\n",
    "for n_es in range(500,1000,100):\n",
    "    for max_f in range(2,5,1):\n",
    "        rf = RandomForestClassifier(n_estimators = n_es,n_jobs =-1,criterion ='entropy',max_features=max_f)\n",
    "        print(\"n_es = %s\"%n_es)\n",
    "        print(\"max_f = %s\"%max_f)\n",
    "        print(cross_val_score(rf,over_san_train,Target_over,cv=10).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Based on the best case achieved above we will build the random forest classifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators =900 ,criterion ='entropy',max_features=18)\n",
    "forest = forest.fit(over_san_train,Target_over)\n",
    "output_over = forest.predict(San_test)\n",
    "\n",
    "#Then to save the predicted output\n",
    "\n",
    "RF_1 = pd.DataFrame({'ID':San_test_id,'TARGET':output_under})\n",
    "\n",
    "os.chdir(r'E:\\Study\\Data Challenges\\kaggle\\Santander') #Changing the Dir to save the ouput file\n",
    "\n",
    "RF_1.to_csv(\"RF_1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
